# Quick Reference: Algorithm Comparison

## Side-by-Side Comparison

### Sklearn Model: TF-IDF + Multinomial Naive Bayes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Text                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Preprocessing                     â”‚
â”‚  â€¢ Remove punctuation                   â”‚
â”‚  â€¢ Convert to lowercase                 â”‚
â”‚  â€¢ Tokenization                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TF-IDF Vectorization                   â”‚
â”‚  â€¢ 1000 most important features         â”‚
â”‚  â€¢ Term frequency scoring               â”‚
â”‚  â€¢ Stop word removal                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multinomial Naive Bayes                â”‚
â”‚  â€¢ Probability scoring                  â”‚
â”‚  â€¢ Category prediction                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: Category + Confidence Scores   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Formula:**
```
P(Category|Text) = P(Category) Ã— âˆ P(word|Category)
                   (With Laplace smoothing)
```

**Pros:** âš¡ Fast, ğŸ’¾ Memory efficient, ğŸ” Interpretable
**Cons:** âŒ Limited context, âŒ Assumes word independence

---

### Transformer Model: BART Zero-Shot Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Text                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tokenization (BPE)                     â”‚
â”‚  â€¢ Byte Pair Encoding                   â”‚
â”‚  â€¢ Special tokens added                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token & Positional Embeddings          â”‚
â”‚  â€¢ Learned vector representations       â”‚
â”‚  â€¢ Position information encoded         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BART Encoder (12 Layers)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Multi-Head Self-Attention       â”‚   â”‚
â”‚  â”‚ (12 parallel attention heads)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                             â”‚
â”‚           â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Feed-Forward Network            â”‚   â”‚
â”‚  â”‚ (Dense layers)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                             â”‚
â”‚           â–¼                             â”‚
â”‚  Layer Normalization & Residual Conn.  â”‚
â”‚  (Repeat 12 times)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Zero-Shot Classification               â”‚
â”‚  For each category:                     â”‚
â”‚  â€¢ Create: "This is about [category]"  â”‚
â”‚  â€¢ Score entailment probability        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: Category + Confidence Scores   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Formula:**
```
Category Score = Entailment(Text, "This is about [Category]")
             = P(Text entails hypothesis for category)
```

**Pros:** ğŸ¯ Accurate, ğŸ§  Semantic understanding, ğŸ¨ Context-aware
**Cons:** ğŸ¢ Slow (100-200ms), ğŸ’¾ High memory, ğŸ” Black-box

---

## When to Use Each Model

### Use Sklearn When:

âœ… **Speed is critical**
- Real-time predictions needed
- High throughput required
- Processing 1000s of articles/minute

âœ… **Limited resources**
- Running on CPU only
- Memory-constrained environment
- Edge devices or serverless functions

âœ… **Interpretability matters**
- Need to understand why prediction was made
- Feature importance analysis required
- Compliance/audit trail needed

âœ… **Simple classification tasks**
- Clear categorical boundaries
- Good training data in target domain
- Speed over accuracy tradeoff acceptable

### Use Transformer When:

âœ… **Maximum accuracy needed**
- Quality is more important than speed
- Complex linguistic patterns in text
- Nuanced category distinctions

âœ… **GPU available**
- Apple Silicon / NVIDIA GPU at hand
- Cloud GPU resources accessible
- Batch processing acceptable

âœ… **Semantic understanding critical**
- Cross-domain generalization needed
- Few-shot learning capability required
- Long contextual dependencies matter

âœ… **Production quality required**
- Mission-critical applications
- Customer-facing results
- Accuracy benchmarks strict

---

## Performance Metrics Comparison

| Metric | Sklearn | Transformer |
|--------|---------|------------|
| **Speed** | ~5-10ms | ~100-200ms |
| **Accuracy** | 85% | 94%+ |
| **Memory** | ~50MB | ~3000MB |
| **GPU Needed** | âŒ No | âœ… Recommended |
| **Training Time** | ~2 minutes | ~1-2 hours |
| **Inference Cost** | Low | High |

---

## Category Support

Both models support these 12 categories:

| Category | Icon | Use Cases |
|----------|------|-----------|
| Business | ğŸ’¼ | Markets, companies, economy |
| Education | ğŸ“ | Schools, universities, learning |
| Entertainment | ğŸ¬ | Movies, music, celebrities |
| Finance | ğŸ’° | Banking, stocks, investments |
| Health | ğŸ¥ | Medicine, wellness, diseases |
| Legal | âš–ï¸ | Laws, courts, regulations |
| Lifestyle | ğŸ¨ | Fashion, travel, culture |
| Politics | ğŸ›ï¸ | Government, elections, policy |
| Science | ğŸ”¬ | Research, discoveries, tech |
| Sports | âš½ | Games, athletes, competitions |
| Technology | ğŸ’» | Software, gadgets, AI |
| World | ğŸŒ | International, global events |

---

## Dataset Statistics

```
Total Samples:        120,577
Training Set (80%):   96,461
Test Set (20%):       24,116
Categories:           12
Average Text Length:  200-500 tokens
Vocabulary Size:      1,000 (TF-IDF)

Category Distribution (approximate):
- Business:        30,000 samples
- Entertainment:   30,000 samples
- Sports:          30,000 samples
- Technology:      30,000 samples
- Politics:           400 samples
- Others:             577 samples
```

---

## Architecture Comparison

### Sklearn Architecture

```
Text Input
    â†“
TF-IDF Vectorizer (1 layer)
    â†“
Multinomial Naive Bayes
    â†“
Probability Distribution
```

**Total Parameters:** ~1M (small)
**Layers:** 1 effective layer
**Training:** Statistical learning

### Transformer Architecture

```
Text Input
    â†“
Embedding Layer
    â†“
Positional Encoding
    â†“
Transformer Encoder (12 layers)
â”œâ”€ Multi-Head Attention (12 heads)
â”œâ”€ Feed-Forward Network
â””â”€ Layer Normalization
    â†“
Output Layer
    â†“
Probability Distribution
```

**Total Parameters:** 406M (large)
**Layers:** 12 transformer layers
**Training:** Deep neural network learning

---

## API Usage Examples

### Sklearn Prediction

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Apple releases new iPhone with AI features",
    "model_type": "sklearn"
  }'

# Response:
# {
#   "category": "Technology",
#   "confidence": 0.92,
#   "model_type": "sklearn",
#   "processing_time": 0.008
# }
```

### Transformer Prediction

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Apple releases new iPhone with AI features",
    "model_type": "transformer"
  }'

# Response:
# {
#   "category": "Technology",
#   "confidence": 0.98,
#   "model_type": "transformer",
#   "processing_time": 0.152
# }
```

---

## Key Differences Summary

| Aspect | Sklearn | Transformer |
|--------|---------|------------|
| **How it works** | Count-based | Neural network-based |
| **Learns** | Word frequencies | Deep representations |
| **Understands** | Bag of words | Semantic relationships |
| **Context** | No (word order doesn't matter) | Yes (full context) |
| **Generalization** | Limited | Excellent |
| **Explainability** | High | Low |
| **Speed** | Blazing fast | Moderate |
| **Accuracy** | Good | Excellent |
| **Best for** | Production systems | Accuracy-critical apps |

---

## Further Reading

- **Full Algorithm Details:** See [MODEL.md](../MODEL.md)
- **API Documentation:** See `/docs` endpoint
- **Source Code:** See `backend/model_service.py`
